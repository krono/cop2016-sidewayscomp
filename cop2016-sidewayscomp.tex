%
%  Fast Sideways Composition, for COP 2016
%
% Confirmation Number:» 9
% Submission Passcode:» n/a
% MAX 6 PAGES
%
\RequirePackage[l2tabu, orthodox]{nag}
\PassOptionsToPackage{final}{graphics}
\documentclass[preprint,english,10pt,nonatbib]{sigplanconf}
\usepackage{myheader}
\addbibresource{references.bib}
\newacronym{api}{\textsc{api}}{application programming interface}
\newacronym{aop}{\textsc{aop}}{aspect-oriented programming}
\newacronym{cop}{\textsc{cop}}{context-oriented programming}
\newacronym{oop}{\textsc{oop}}{object-oriented programming}
\newacronym{oo}{\textsc{oo}}{object-oriented}
\newacronym{dsl}{\textsc{dsl}}{domain-specific language}
\newacronym{jit}{\textsc{jit}}{just-in-time}
\newacronym{vm}{\textsc{vm}}{virtual machine}
\newacronym{ffi}{\textsc{ffi}}{foreign-function interface}
\DeclareRobustCommand*\copa{\textsc{cop}\kern .12em\oldstylenums{09}\kern .12em a\xspace}
\DeclareRobustCommand*\copb{\textsc{cop}\kern .12em\oldstylenums{09}\kern .12em b\xspace}
\DeclareRobustCommand*\deltablue{\textsc{DeltaBlue}\xspace}

\begin{document}
\conferenceinfo{COP '16}{July 19, 2016, Rome, Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/16/07}
\copyrightdoi{nnnnnnn.nnnnnnn}

\publicationrights{licensed}     % this is the default

\titlebanner{DRAFT \--- not for distribution \--- DRAFT}        % These are ignored unless
\preprintfooter{Submitted to COP'16}

\title{Optimizing Sideways Composition}
\subtitle{Fast Context-oriented Programming in ContextPyPy}

% \authorinfo{Tobias Pape}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tobias.pape@hpi.uni-potsdam.de}
% \authorinfo{Tim Felgentreff}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tim.felgentreff@hpi.uni-potsdam.de}
% \authorinfo{Carl Friedrich Bolz}
%            {King's College, London}
%            {cfbolz@gmx.de}
\authorinfo{Tobias Pape\textsuperscript{*} \and Tim Felgentreff\textsuperscript{*\textdagger} \and Robert Hirschfeld\textsuperscript{*\textdagger}}
           {\textsuperscript{*}Hasso Plattner Institute, University of Potsdam, Germany\\
           \textsuperscript{\textdagger}Communications Design Group (CDG), SAP Labs, USA\\
           \textsuperscript{\textdagger}Viewpoints Research Institute, USA}
           {\{firstname\}.\{lastname\}@hpi.uni-potsdam.de}

\maketitle

\begin{abstract}
  The prevalent way of code sharing in many current object systems is static
  and/or single inheritance; both are limiting in situations that call for
  multi-dimension decomposition. Sideways composition provides a technique to
  reduce their limitations. \Ac{cop} notably applies sideways composition to
  achieve better modularity. However, most \ac{cop} implementations have a
  substantial performance overhead. This is partly because weaving and
  execution of layered methods violate assumptions that common language
  implementations hold about lookup. Meta-tracing \ac{jit} compilers have
  unique characteristics that can alleviate the performance overhead, as they
  can treat lookup differently.

  We show that meta-tracing \ac{jit} compilers are good at optimizing
  sideways composition and give initial, supporting results. Furthermore, we
  suggest that explicit communication with the \ac{jit} compiler in a \ac{cop}
  implementation can improve performance further.
  \end{abstract}

\category{D.3.3}{Programming Languages}{Language Constructs and Features}
\category{D.3.2}{Programming Languages}{Language Classifications}[Mul\-ti\-par\-a\-digm languages]
\category{D.3.4}{Programming Languages}{Processors}[Run-time environments]

\keywords
context-oriented programming,
meta-tracing \acs{jit} compilers,
optimization,
virtual machines,
PyPy

\section{Introduction}

Sideways composition provides a technique to avoid some of the limitations of
static, single inheritance object-oriented systems where multi-dimensional
composition of behavior is desirable. In particular, \acrlong{cop} applies
sideways composition to improve modularity. \ac{cop} as a modularity mechanism
to dynamically adapt behavior at run-time has been demonstrated to be useful in
a variety of scenarios. Beyond its original motivation for dynamically adapting
systems based on environmental factors such as battery level, geolocation, or
time of day~\cite{hirschfeld+:2008:context-oriented-programming}, \ac{cop} has
also been applied to provide safety in the development of live
systems~\cite{lincke+:2012:scoping-changes} or to let multiple conflicting
versions of programming interfaces
co-exist~\cite{felgentreff:2013:tool-building}.

However, as with many abstraction mechanisms, \ac{cop} comes with some
overhead, a fact that has been repeatedly recognized. By weaving and executing
layered methods, it violates assumptions of common language implementations and
thus context layer aware method lookup requires additional operations at
run-time. Most \ac{cop} implementations in dynamic languages use the host
language's meta-programming facilities to redirect method dispatch, whereas
statically compiled languages require additional compilation steps to construct
data structures to track layer activation states at run-time. Both of these
solutions come with considerable performance decrease from
\SIrange{75}{99}{\percent}~\cite{appeltauer+:2009:comparison-context-oriented}.

We argue that \acp{vm} with meta-tracing \ac{jit} compilers can alleviate the
performance overhead, because their execution model can allows them to optimize
non-standard lookup. This can be achieved by explicitly communicating to the
\ac{jit} complier crucial information using small ``hints'' on the side of the
\ac{cop} implementation, avoiding a complex architecture to cache and optimize
lookup.

% \paragraph*{Contributions}

In this work, we make the following contributions and show
\begin{itemize}
\item that the performance overhead of sideways composition is still
  present in most \ac{cop} implementations;
\item that meta-tracing--based \ac{jit} compilation can reduce the
  overhead of sideways composition; and
\item that communicating the active layer composition \emph{explicitly} to the
  \ac{jit} compiler can further reduce the overhead of sideways composition.
\end{itemize}


\section{Background: Meta-tracing \acs{jit} Compilers}

\Acf{jit} compilation is one of the most used technique for speeding up the
execution of programs at run-time. Many language implementations have yet
benefitted from \ac{jit} compilers, including but not limited to today's
popular languages such as Java~\cite{paleczny+:2001:java-hotspot} or
JavaScript~\cite{holtta:2013:crankshafting-from}. A particular implementation
approach for \ac{jit} compilers is to us \emph{tracing}, that is, recoding the
steps an interpreter takes to obtain its instruction sequence, a \emph{trace}.
This trace is subsequently used instead of the interpreter to execute the same
part of that program~\cite{mitchell:1970:design-construction} at higher speed.
% Native code~\cite{bala+:2000:dynamo:-transparent} as well as object-oriented
% programs~\cite{gal+:2006:hotpathvm:-effective} have already been optimized
% using this technique.

With \emph{meta-tracing} the execution of the interpreter is observed instead
of the execution of the application program. The resulting traces are therefore
not specific to the particular application but to the underlying
interpreter~\cite{bolz+:2009:tracing-meta-level:}. This way, language
implementers do not have to implement optimized language-specific \ac{jit}
compilers but rather to provide a straightforward language-specific interpreter
that is subject to the meta-tracing technique. ``Hints'' enable
communication with and hence fine-tuning of \ac{jit}
compiler~\cite{bolz_runtime_2011}.

\section{Faster Sideways Composition with Meta-tracing}

We propose that meta-tracing \ac{jit} compilers can reduce the overhead of
sideways composition and \ac{cop}, and that communication the composition of
active layers to the \ac{jit} compiler can reduce the overhead even further.

\subsection{Employing a Meta-tracing \protect\acs{jit}}
\Acl{cop} employs sideways composition to inject context-dependent behavioral
changes into an existing hierarchy of behavior. These hierarchies are typically
defined by the static and/or single inheritance of object systems. These
hierarchies are typically important for execution time performance, as they
form the basis for \emph{lookup}.

Most execution environments, such as \acp{vm} for dynamic object-oriented
languages, assume that those hierarchies change rarely and hence lookup can be
fast. However, using sideways composition to alter behavior invalidates this
assumption. Especially, since \ac{cop} explicitly redefines lookup based on
currently active layers; the composition of currently active layers becomes
\emph{crucial} for calculating the lookup in the dynamic extent of executed
code. If this composition stays the same, lookup stays the same, if it changes,
lookup may change. Execution environments typically have to decide, whether to
always re-exercise the lookup for every method under the active layers, or
cache (and invalidate) lookup information when the composition of active layers
changes. For example in the sequence
\begin{lstlisting}
-- active layers: §\(\emptyset\)§
method1()
  activate(layer1)
    method2()
  deactivate(layer1)
\end{lstlisting}
a \ac{cop} implementation typically uses one of the following two strategies:
\begin{enumerate}
\item lookup \lstinline|method1()| under the composition
  \(\emptyset\)
  and lookup \lstinline|method2()| under the composition
  [\lstinline|layer1|], or
\item use cached lookup information for \lstinline|method1()|, switch cached
  lookup information due to change in active layers, use (new) cached lookup
  information for \lstinline|method2()|.
\end{enumerate}
Both cases effect a performance impact either on every lookup or on every
layer change.

With meta-tracing \ac{jit} compilers, however, this effect is much less
severe. Although rarely-changing lookup still is helpful for its operation, a
change in lookup \--- for example induced by a layer activation \--- can be
anticipated and be accounted for.

Thus, with a properly instructed meta-tracing \ac{jit} compiler, a third option
becomes available. At points in the execution where the composition of active
layers becomes important, a \emph{guard} ensures that this composition did not
change. While counter-intuitive at first, this actually is a benefit. When a
certain different composition has been encountered often enough at the guard,
the meta-tracing \ac{jit} compiler will introduce a \emph{bridge} into a new
part of a trace, in which \emph{this} different composition can be assumed not
to change, and lookup can be optimized accordingly. Note that this resembles
strategy 2 above, but is implicit and actually guided by the \ac{jit} compiler.
Therefore, the actual \ac{cop} implementation does not have to manage the
caching information when altering the lookup information, saving both execution
time and implementation complexity.

\subsection{Promoting the Compositions of Active Layers}

The compositions of active layers is crucially important for the lookup in
\ac{cop}, and all strategies above reflect this. However, only the
language-level implementation of \ac{cop} actually knows about the
composition's importance, and even for meta-tracing \ac{jit} compiler's
strategy, the \ac{jit} compiler first hast to become aware of the fact that the
composition is \--- indeed \--- important for its trace. Yet, the \ac{jit}
compile can only apply heuristics to identify the composition as
trace-important.

This situation is commonly known when implementing \acp{vm} that use a
meta-tracing \ac{jit} compiler. Based on the value of a certain object it may
be desirable to \emph{specialize} traces to these values (essentially what
happened above with the guard and the bridge).
Implementers can chose to \emph{promote}~\cite[\S
3.1]{bolz+:2011:runtime-feedback} such an object and the \ac{jit} compiler will
ensure that traces are actually specialized to the object's values,  regardless
of wether the \ac{jit} compiler's \emph{heuristics} would result in the same
specialization or not. If applied carefully, this promotion can decrease
execution time.

Up until recently, this \emph{promotion} of objects had not been available to
language-level implementers of \ac{cop}. However, at the time of writing, one
\ac{vm} with meta-tracing \ac{jit} compiler (PyPy) exposes the \emph{promote}
functionality to the language-level and it is possible to use it for a \ac{cop}
implementation. The composition of active layers can now be promoted and the
meta-tracing \ac{jit} compiler now ensures that (a) a specialized traces exists
for each encountered composition, and (b) within a given trace, the composition
wont change and can be relied upon. This assumption now can be made when
exercising lookup during execution, saving execution time.

\begin{figure*}[t]
  \centering
  \hbox to \linewidth{\hss
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-3}
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-2}
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-4}
  \hss}
\caption{Results of \copa. Relative throughput of method execution in
  \protect\acs{cop} implementations with (each left to right) 0 to 10 layers
  normalized to the respective non-layered workload. Higher is better. For raw
  numbers see \autoref{tab:copa} and \autoref{tab:copall}. (Note
  the different scales, see \autoref{fig:malte-a-overview} for an overview
  comparison.)}
  \label{fig:malte-a}
\end{figure*}


\section{ContextPyPy Implementation Outline}
\begingroup
\lstset{language=Python}

We briefly describe certain implementation details of ContextPy and how
``hints'' to the meta-tracing \ac{jit} compiler lead to ContextPyPy.

During development, programmers using ContextPy can annotate methods as being
advices before/after/around their base methods in a certain layer
(Layer-in-Class). Using Python annotations, these methods are registered to a
\emph{descriptor} holding onto all partial method and possibly a base method. That way
only methods that are layered at least once are affected.

The descriptor, being a Python callable, takes the place of the named method in
the class and, from this point on, dictates execution and effectively lookup of
the partial methods. When code calls the method \--- now represented by the
descriptor \--- the active layer composition is determined. This considers
globally activated (\lstinline|globalActivateLayer| plus counterpart) layers
but also layers active in the dynamic extent. For that, a thread local storage
provides a dynamic scope for the layer composition, which can be affected using
Python's \lstinline|with| syntax (\lstinline|with activeLayers(myLayer): ...|).



%


\section{Performance Evaluation with \acs{cop}}


As described in the introduction, we evaluate the following:
\begin{enumerate}
\item Sideways composition \emph{still} has a considerable impact on execution
  time.
\label{item:1}
\item Meta-tracing \ac{jit} compilers can alleviate the performance impact of
  sideways composition on execution time.
\label{item:2}
\item Explicit \emph{promotion} of the active layer composition can further
  improve execution time.
\label{item:3}
\end{enumerate}
%
For \ref{item:1}, we re-run in parts benchmarks presented in
2009~\cite{appeltauer+:2009:comparison-context-oriented}, which showed a
performance impact of sideways composition as used with \ac{cop}
implementations. For \ref{item:2}, we additionally augment the well-known
DeltaBlue benchmark~\cite{freeman-benson+:1989:deltablue-algorithm:} with
layers and additional functionality and compare the impact of sideways
composition on platforms with and without meta-tracing \ac{jit} compilers.
For \ref{item:3}, we run both benchmarks with a \emph{promote}-enhanced
\ac{cop} implementation, as well.
We present and discuss the most important results for each benchmark and
provide all results in the appendix.

\subsection{Setup}
\label{sec:setup}


\paragraph{System} We executed the benchmarks on %
% MacBook Pro Tobi
an Intel Core i7-4850HQ at \SI{2.3}{\GHz} with \SI{6}{\mega\byte} cache and
\SI{16}{\giga\byte} of RAM. The machine ran Mac OS\,X 10.9.5.
%%%% Surface Tim??
Certain benchmarks were run on an Intel Core i7-4650U at \SI{1.7}{\GHz} with
\SI{4}{\mega\byte} cache and \SI{8}{\giga\byte} of RAM, this machine ran
Windows 10.
%
%%%% Maxinchen
% an Intel Xeon E5-2650 (Sandy Bridge)  at \SI{2.8}{\GHz} with
% \SI{20}{\mega\byte} cache and \SI{16}{\giga\byte} of RAM. 
% Although virtualized on Xen, the machine was idle.
% The machine ran Ubuntu 14.04.1 LTS with a \SI{64}{\bit} Linux 3.2.0.

\paragraph{Implementations} %

We used ContextPy\footnote{{\smaller\url{https://bitbucket.org/cfbolz/contextpy/}}
  rev 5155cb7.} with Python 2.7.5 and PyPy 5.1 on OS\,X and
ContextPyPy\footnote{ContextPy as above but with explicit \emph{promote}.} with
PyPy 5.1 on OS\,X in both benchmarks. Additionally, for the re-run of the 2009
benchmark, we used
ContextJS~\cite{lincke+:2011:open-implementation}\footnote{{\smaller\url{https://github.com/LivelyKernel/LivelyKernel/tree/master/core/cop}}
  rev 0bd117c.} with V8 (Chrome 50.0.2661.66 beta 64-bit) on OS\,X, Chakra
(Edge 25.10586) on Windows 10 and V8 (Chrome 49.0.2623.112) on Windows 10; as
well as LispWorks\textsuperscript{\textregistered} 64-bit 7.0.0 on OS\,X.

Introducing layers with ContextPy increases the initial size of the traces
beyond PyPy's standard maximum trace limit. Therefore, we use a slightly
altered build of PyPy 5.1 that allows a much higher trace limit than the
standard build.


\paragraph{Methodology} 
% INITSIZE = 200000
% MAXSIZE = 1000000000
% TARGETTIME = 5.0 * 1000
For \copa and \copb (see \autoref{sec:benchmarks}), every benchmark was run with increasing size until a
measurement took at least 5 seconds; this matches the original
methodology~\cite{appeltauer+:2009:comparison-context-oriented}. Warm-up is
provided by the not measured runs. 
% ITER_DEFAULT=${ITER:-5}
% SIZE_DEFAULT=${SITE:-18000}
% WARM_DEFAULT=${WARM:-3}
For \deltablue (see \autoref{sec:benchmarks}), every benchmark was run 5 times
uninterrupted in a new process with additional 3 times warm-up prior to
measurement for PyPy\footnote{Python did not exhibit any warm-up--related
  differences/}. The execution time was always measured \emph{in-system} and,
hence, does not include start-up. We show the arithmetic mean of all runs along
with bootstrapped~\cite{davison+:1997:confidence-intervals} confidence
intervals for a \SI{95}{\percent} confidence level.

%\footnote{Raw figures can be  found as an appendix (\autoref{tab:all})}

% \paragraph{Availability}  All of our benchmarks and infrastructure
%  are available at <<TBD>>.


\subsection{Benchmarks}
\label{sec:benchmarks}

Our first set of benchmarks is taken from one of our earlier
papers~\cite{appeltauer+:2009:comparison-context-oriented}. These
micro-benchmarks attempt to measure the pure performance overhead of
dispatching to active layers and of layer activation. In the first part of
these benchmarks (\copa), we use an object with ten integer variables
(\texttt{counter\textsubscript{1}} to \texttt{counter\textsubscript{10}}) that
provides ten methods (\texttt{method\textsubscript{1}} to
\texttt{method\textsubscript{10}}), where each \texttt{method\textsubscript{i}}
increments all counters from \texttt{counter\textsubscript{1}} to
\texttt{counter\textsubscript{i}} by one. The same behavior is provided by a
method \texttt{layered}. The base method increments only
\texttt{counter\textsubscript{1}}, and nine layers
(\texttt{Layer\textsubscript{1}} to \texttt{Layer\textsubscript{9}}) provide a
partial method to adapt the base method to each increment one of the other
counters. Running just the \texttt{layered} method without any layers being
active thus yields the same behavior as \texttt{method\textsubscript{1}}.


In the second part (\copb) of our first benchmark set, we measure the
performance impact of layer activation. For most \ac{cop} languages, layer
activation means updating internal data structures with the current layer
composition. To quantify this impact, we measured the execution time of running
five methods (\texttt{method\textsubscript{1}} to
\texttt{method\textsubscript{5}}) in succession that each increment one
counter. We compare this to the execution time of five partial methods from
five layers that implement the same method body, where each layer is activated
in succession.


\def\idBox#1#2{%
\setlength{\fboxsep}{1pt}%
% \colorbox[HTML]{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\colorbox{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\xspace}
% DeltaBlue~\idBox{blue}{\(\circ\)}, %
% DeltaPurple ~\idBox{purple}{\(\bigtriangleup\)}, %
% DeltaViolet ~\idBox{violet}{\(\square\)}, %
% DeltaRed~\idBox{red}{\(+\)}, %
%  % ~\idBox{}{\(\boxtimes\)}, %
%  % ~\idBox{E41A1C}{\(\)}, %

Our second benchmark (\deltablue) measures the relative overhead of sideways
composition on a more wide-spread benchmark, DeltaBlue. For that, we augmented
DeltaBlue with additional functionality (reversed lists for storage, count of
constraint executions and binary constraints), both statically added to the
benchmark (Delta\emph{Purple}~\idBox{purple}{\(\bigtriangleup\)})
and dynamically composed via layers (Delta\emph{Red}~\idBox{red}{\(+\)}).
To measure the overhead of the mere presence of dynamic sideways composition
functionality without actually using it, we also measured DeltaRed with all
layers deactivated (Delta\emph{Violet} ~\idBox{violet}{\(\square\)}).
We normalize the execution time of all four benchmarks to
Delta\emph{Blue}~\idBox{blue}{\(\circ\)}
on each implementation and \ac{vm}. That way, we show the overhead of the
\ac{cop} implementation rather than mere execution time.

% All benchmarks are single-threaded.

\subsection{Results}

\begin{figure*}
  \centering
  \includegraphics[width=.85\linewidth]{bench/malte-b}
  \caption{Results of \copb. Relative throughput of layer activation in
    \protect\acs{cop} implementations with (each left to right) 1 to 5 layers
    normalized to a workload with no layers active whatsoever. Higher is
    better. For raw numbers see \autoref{tab:copb} and \autoref{tab:copall}.}
  \label{fig:malte-b}
\end{figure*}

The results of the \copa benchmark are shown in \autoref{fig:malte-a}. We
evaluated the performance by comparing each ordinary method
\texttt{method\textsubscript{i}} with the execution performance of activating
all layers from \texttt{Layer\textsubscript{1}} to
\texttt{Layer\textsubscript{i}} (which gives the same behavior) and normalizing
to the ordinary method. As we reported in our previous work, ContextL shows a
performance degradation ranging from \SIrange{22}{65}{\percent}. Interestingly,
the decrease in performance does not seem to correspond to the number of active
layers, likely due to the variability of the optimizations of the underlying
Lisp \ac{vm}. On the V8 and Chakra JavaScript \acp{vm}, ContextJS incurs a
massive performance hit of over \SI{99.7}{\percent} in all cases, even where no
layer is active. ContextPy on the Python \ac{vm} is little better with overhead
around \SI{95}{\percent}.

Running ContextPy on PyPy, that is, using a meta-tracing \ac{jit} compiler,
yields vastly different results. Against the trend of the other
implementation/platform combinations, using \ac{cop} layers can actually
\emph{increase} performance. When just layers are present but not activated,
ContextPy on PyPy can gain a \(3.5\times\) speedup and ContextPyPy even over
\(4\times\). Compared with ContextPy on Python, the relative performance
difference is up to two orders of magnitude improved.

Moreover, ContextPyPy manages to retain a speedup of at least \(2\times\)
over the non-layered version up to three active layers, exhibiting only minor
slowdown for four to six active layers. After that, the performance levels up
with ContextPy on PyPy. We attribute the latter, rather steep decline in
performance to how the meta-tracing \ac{jit} compiler handles rather long
traces that can occur with an increasing number of active layers and which are
yet to be investigated.

\paragraph{Discussion} The meta-tracing \ac{jit} compiler of PyPy appears to be
effective at eliminating the overhead of sideways composition. The results of
ContextPyPy suggest that communicating layer information properly can vastly
improve this effect, too.
%

\medskip\noindent %
The results of the \copb benchmark are shown in \autoref{fig:malte-b}. The
performance impact for layer activation for each of the tested systems is
comparable and clearly increases as more layers are activated. Moreover,
ContextPy on Python \--- the only implementation/platform combination without
\ac{jit} or \ac{jit}-like optimizations \--- exhibits the least severe impact
with increasing layers.

\paragraph{Discussion} The meta-tracing \ac{jit} compiler of PyPy seems to have
to invalidate assumptions on layer activations, possibly not being able to
re-use certain traces. The results of ContextPyPy suggest that the current way
of communicating layer information to the meta-tracing \ac{jit} compiler in
fact can also hamper optimizations, that is, such \emph{hints} have to
be used with great care.

\begin{figure*}
  \centering
  \includegraphics[width=.55\linewidth]{bench/DeltaBlue-norm.pdf}
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue-norm.pdf}
  % \hfil
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue.pdf}
  \caption{Results of \deltablue. Relative execution time of Delta\ldots benchmarks
    on ContextPy on Python and PyPy; and ContextPyPy. Normalized to DeltaBlue.
    Lower is better. For raw numbers see \autoref{tab:deltablue-raw}.}
  \label{fig:deltablue}
\end{figure*}

\medskip\noindent %
The results of the \deltablue benchmark are shown in \autoref{fig:deltablue}.
Expectedly, the additional workload of DeltaPurple over DeltaBlue is comparable
across all three implementation/platform combinations. However, the mere
presence of layered methods (DeltaViolet) has a comparatively high impact on
Python, with up to \(3\times\)
slowdown. At the same time, virtually no overhead is present on PyPy for
DeltaViolet. The overhead of activated layers (DeltaRed) is quite severe on
Python, ranging about \(5\times\)
of DeltaPurple, whereas on PyPy the slowdown is less than \SI{50}{\percent}
(\(\approx 1.4\times\)). ContextPyPy performs virtually the same as ContextPy
on PyPy.

\paragraph{Discussion} \deltablue, being a less ``micro'' benchmark, reinforces
the impression of \copa than the meta-tracing \ac{jit} compiler of PyPy is
effective at eliminating the overhead of sideways composition. Especially the
no-layers-activated case having no overhead seems important for wider adoption.
Seeing \copa and \copb, the indifference between ContextPy on PyPy and
ContextPyPy is unexpected, both the missing speedup from \copa and the missing
slowdown from \copb. However, the slowdown of \copb is neither present for
ContextPy on PyPy, suggesting that ContextPyPy's \emph{promote} approach could
be nevertheless viable to use.
% \subsection{Threats to Validity}

% OSX: SpeedStep cannot be disabled.

\section{Related Work}

Other implementations of \acrlong{cop} have tried to optimize their
implementations using traditional compiler and optimization techniques. However,
while they can only reduce the performance overhead in some cases, they are
sometimes difficult to apply and increase the complexity of the system. A common
optimization approach is to shift the performance impact to the layer activation
time under the assumption the changes in layer composition are comparatively
rarer than execution of layered methods. This approach can drastically limit the
performance impact of layers when the composition changes rarely, but at the
cost of reduced performance for applications where the layer composition may
change frequently.

Instances where this optimization is used are ContextAmber, Elektra, and
cj. ContextAmber~\cite{springer2015efficient} optimistically flattens layered
methods when the layer composition changes to achieve near native performance
during execution. An extension to the C++ configuration management system
Elektra~\cite{Raab:2014:PEE:2637066.2637074} make use of extensive code
generation and caching of the active layer composition to minimize the
performance impact of running with active layers in a tight loop. The
cj\cite{schippers2009implementation,schippers2008delegation} system that
implements \ac{cop} on top of the delMDSOC virtual machine model. This model is
well suited towards multi-dimensional dispatch, and thus a \ac{cop}
implementation on top of it achieves good performance in this case. However, as
with the other two approaches, switching layers becomes more expensive as a
result.

Another approach at optimizing \ac{cop} that is closer to the work presented
here is to use facilities of general purpose \acp{vm} directly. One such
facility is Java's INVOKEDYNAMIC instruction that allows language extensions to
implement new lookup semantics for the Java \ac{vm}. However, prior
work~\cite{appeltauer2010layered} indicates that this extension point may
provide only minimally improved performance compared to an implementation using
language-level caching facilities, albeit using less code and a simplified
architecture.

\section{Conclusion and Future Work}

Our first results for using meta-tracing \ac{jit} optimizations to reduce the
performance impact of \ac{cop} are promising for micro benchmarks. The
meta-tracing \ac{jit} compiler of PyPy appears to be effective at eliminating
the overhead of sideways composition for method lookup. Our results also show
that a few, carefully placed hints can help the runtime to improve this effect,
too.

For future work we have to further investigate how careful one has to be in
placing those hints and how the performance behavior changes with larger
applications. Our results with a larger benchmark indicate that the approach is
viable, but so far the performance is less than we had hoped for. Nonetheless
the fact that we see no overhead with our approach for executing layered methods
when no or only one layer is active and the layer composition does not change
already shows that \ac{cop} can be enabled in a language without paying a
performance penalty. We think this result in itself is important for to argue
for wider adoption of sideways composition mechanisms like \ac{cop}.

\acks
We gratefully acknowledge the financial support of HPI's Research School and
the Hasso Plattner Design Thinking Research Program (HPDTRP).
% Carl Friedrich
% Bolz is supported by the EPSRC \emph{Cooler} grant EP/K01790X/1.
LispWorks Ltd. kindly provided an evaluation license of
LispWorks\textsuperscript{\textregistered} 64-bit for Mac OS\,X for testing
purposes. We greatly appreciate the support of Carl Friedrich Bolz with the
ContextPy and ContextPyPy implementations.
\printbibliography
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}
\section{Comprehensive Results}
\input{deltablue-raw}
\input{malte-raw}
\end{document}
