%
%  Fast Sideways Composition, for COP 2016
%
% Confirmation Number:» 9
% Submission Passcode:» n/a
% MAX 6 PAGES
%
\RequirePackage[l2tabu, orthodox]{nag}
\PassOptionsToPackage{final}{graphics}
\documentclass[preprint,english,10pt,nonatbib]{sigplanconf}
\usepackage{myheader}
\addbibresource{references.bib}
\newacronym{api}{\textsc{api}}{application programming interface}
\newacronym{aop}{\textsc{aop}}{aspect-oriented programming}
\newacronym{cop}{\textsc{cop}}{context-oriented programming}
\newacronym{oop}{\textsc{oop}}{object-oriented programming}
\newacronym{oo}{\textsc{oo}}{object-oriented}
\newacronym{dsl}{\textsc{dsl}}{domain-specific language}
\newacronym{jit}{\textsc{jit}}{just-in-time}
\newacronym{vm}{\textsc{vm}}{virtual machine}
\newacronym{ffi}{\textsc{ffi}}{foreign-function interface}
\DeclareRobustCommand*\copa{\textsc{cop}\,\oldstylenums{09}\,a\xspace}
\DeclareRobustCommand*\copb{\textsc{cop}\,\oldstylenums{09}\,b\xspace}
\DeclareRobustCommand*\deltablue{\textsc{DeltaBlue}\xspace}

\begin{document}
\conferenceinfo{COP '16}{July 19, 2016, Rome, Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/16/07}
\copyrightdoi{nnnnnnn.nnnnnnn}

\publicationrights{licensed}     % this is the default

\titlebanner{DRAFT \--- not for distribution \--- DRAFT}        % These are ignored unless
\preprintfooter{Submitted to COP'16}

\title{Optimizing Sideways Composition}
\subtitle{Fast Context-oriented Programming in ContextPyPy}

% \authorinfo{Tobias Pape}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tobias.pape@hpi.uni-potsdam.de}
% \authorinfo{Tim Felgentreff}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tim.felgentreff@hpi.uni-potsdam.de}
% \authorinfo{Carl Friedrich Bolz}
%            {King's College, London}
%            {cfbolz@gmx.de}
\authorinfo{Tobias Pape\textsuperscript{*} \and Tim Felgentreff\textsuperscript{*\textdagger} \and Robert Hirschfeld\textsuperscript{*\textdagger}}
           {\textsuperscript{*}Hasso Plattner Institute, University of Potsdam, Germany\\
           \textsuperscript{\textdagger}Communications Design Group (CDG), SAP Labs, USA\\
           \textsuperscript{\textdagger}Viewpoints Research Institute, USA}
           {\{firstname\}.\{lastname\}@hpi.uni-potsdam.de}

\maketitle

\begin{abstract}
  The prevalent way of code sharing in most of the current object systems is
  static and/or single inheritance, both are limiting. Sideways composition
  provides a technique to reduce their limitations. \Ac{cop} notably applies
  sideways composition to achieve better modularity. However, most \ac{cop}
  implementations have a substantial performance overhead. This is partly
  because weaving and execution of layered methods violate common assumptions
  about lookup. Meta-tracing \ac{jit} compiler have unique characteristics that
  can alleviate the performance overhead, as they can treat lookup
  differently.\\
  We suggest that meta-tracing \ac{jit} compilers are good at optimizing
  sideways composition and give initial results. Furthermore, we show that
  hints for the \ac{jit} compiler in a \ac{cop} implementation can improve
  performance further.
  \end{abstract}

\category{D.3.3}{Programming Languages}{Language Constructs and Features}
\category{D.3.2}{Programming Languages}{Language Classifications}[Mul\-ti\-par\-a\-digm languages]
\category{D.3.4}{Programming Languages}{Processors}[Run-time environments]

\keywords
context-oriented programming,
meta-tracing \acs{jit} compilers,
optimization,
virtual machines,
PyPy

\section{Introduction}



% \paragraph*{Contributions}

In this work, we make the following contributions and show
\begin{itemize}
\item that the performance overhead of sideways composition is still
  present in most \ac{cop} implementations;
\item that meta-tracing--based \ac{jit} compilation can reduce the
  overhead of sideways composition; and
\item that communicating the active layer composition \emph{explicitely} to the
  \ac{jit} compiler can further reduce the overhead of sideways composition.
\end{itemize}


\section{<<Background: COP, COPSurvey, MetaTracing>>}

\subsection{The Performance of \acs{cop}}
\label{sec:performance-cop}
\ac{cop} as a modularity mechanism to dynamically adapt behavior at run-time has
been demonstrated to be useful in a variety of scenarios. Beyond its original
motivation for dynamically adapting systems based on environmental factors such
as battery level, geolocation, or time of day~\todo{cite}, \ac{cop} has also
been applied to provide safety in the development of live
systems~\cite{lincke+:2012:scoping-changes} or to let multiple conflicting
versions of programming interfaces co-exist~\todo{cite my talk cop 2013}.

~\cite{lincke+:2011:implementing-scoped}

However, as with many abstraction mechanisms, \ac{cop} comes with some overhead,
a fact that has been repeatedly recognized. Context layer aware method lookup
requires additional operations at run-time. Most \ac{cop} extensions to dynamic
languages use the host languages meta-programming facilities to redirect method
dispatch, whereas statically compiled languages require additional compilation
steps to construct data structures to track layer activation states at
run-time. Both of these solutions come with considerable performance decrease
from \SIrange{75}{99}\percent~\cite{appeltauer+:2009:comparison-context-oriented}.

So far, attempts to apply traditional compiler and optimization techniques have
proven to be difficult and can only reduce the performance overhead in some
cases. A common approach is to shift the performance impact to the layer
activation time. ContextAmber~\cite{springer2015efficient} optimistically
flattens layered methods when the layer composition changes to achieve near
native performance during execution. An extension to the C++ configuration
management system Elektra~\cite{Raab:2014:PEE:2637066.2637074} make use of
extensive code generation and caching of the active layer composition to
minimize the performance impact of running with active layers in a tight
loop. Both approaches nearly eliminate the performance impact of layers when the
composition changes rarely, but at the cost of reduced performance for
applications where the layer composition may change frequently. \ac{vm}
facilities such as Java's INVOKEDYNAMIC instruction provide only minimally
improved performance compared to an implementation using language-level caching
facilities, albeit using less code and a simplified architecture.

\subsection{Meta-Tracing JIT}
\label{sec:meta-tracing-jit}

\section{<<Solution: Sideways-composition and metatracing>>}

We propose that meta-tracing \ac{jit} compilers can reduce the overhead of
sideways composition and \ac{cop}, and that \emph{promoting} the composition of
active layers can reduce the overhead even further.

\subsection{Employing a meta-tracing \protect\acs{jit}}
\Acl{cop} employs sideways composition to inject context-dependent behavioral
changes into an existing hierarchy of behavior. These hierarchies are typically
defined by the static and/or single inheritance of object systems. These
hierarchies are typically important for execution time performance, as they
form the basis for \emph{lookup}.

Most execution environments, such as \acp{vm} for dynamic object-oriented
languages, assume that those hierarchies change rarely and hence lookup can be
fast. However, using sideways composition to alter behavior invalidates this
assumption. Especially, since \ac{cop} explicitly redefines lookup based on
currently active layers; the composition of currently active layers becomes
\emph{crucial} for calculating the lookup in the dynamic extent of executed
code. If this composition stays the same, lookup stays the same, if it changes,
lookup may change. Execution environments typically have to decide, whether to
always re-exercise the lookup for every method under the active layers, or
cache (and invalidate) lookup information when the composition of active layers
changes. For example in the sequence
\begin{lstlisting}
-- active layers: §\(\emptyset\)§
method1()
  activate(layer1)
    method2()
  deactivate(layer1)
\end{lstlisting}
a \ac{cop} implementation typically uses one of the following two strategies:
\begin{enumerate}
\item run the lookup for \lstinline|method1()| under the active layers
  \(\emptyset\) and the lookup for \lstinline|method()| under the active layers
  \([\)\lstinline|layer1|\(]\), or
\item use cached lookup information for \lstinline|method1()|, switch cached
  lookup information due to change in active layers, use (new) cached lookup
  information for \lstinline|method2()|.
\end{enumerate}
Both cases effect a performance impact either on every lookup or on every
layer change.

With meta-tracing \ac{jit} compilers, however, this effect is much less
severe. Although rarely-changing lookup still is helpful for its operation, a
change in lookup \--- for example induced by a layer activation \--- can be
anticipated and be accounted for.

Thus, with a properly instructed meta-tracing \ac{jit} compiler, a third option
becomes available. At points in the execution where the composition of active
layers becomes important, a \emph{guard} ensures that this composition did not
change. While counter-intuitive at first, this actually is a benefit. When a
certain different composition has been encountered often enough at the guard,
the meta-tracing \ac{jit} compiler will introduce a \emph{bridge} into a new
part of a trace, in which \emph{this} different composition can be assumed not
to change, and lookup can be optimized accordingly. Note that this resembles
strategy 2 above, but is implicit and actually guided by the \ac{jit} compiler.
Therefore, the actual \ac{cop} implementation does not have to manage the
caching information when altering the lookup information, saving both execution
time and implementation complexity.

\subsection{Promoting the compositions of active layers}

The compositions of active layers is crucially important for the lookup in
\ac{cop}, and all strategies above reflect this. However, only the
language-level implementation of \ac{cop} actually knows about the
composition's importance, and even for meta-tracing \ac{jit} compiler's
strategy, the \ac{jit} compiler first hast to become aware of the fact that the
composition is \--- indeed \--- important for its trace. Yet, the \ac{jit}
compile can only apply heuristics to identify the composition as
trace-important.

This situation is commonly known when implementing \acp{vm} that use a
meta-tracing \ac{jit} compiler. Based on the value of a certain object it may
be desirable to \emph{specialize} traces to these values (essentially what
happened above with the guard and the bridge).
Implementers can chose to \emph{promote}~\cite[\S
3.1]{bolz+:2011:runtime-feedback} such an object and the \ac{jit} compiler will
ensure that traces are actually specialized to the object's values,  regardless
of wether the \ac{jit} compiler's \emph{heuristics} would result in the same
specialization or not. If applied carefully, this promotion can decrease
execution time.

Up until recently, this \emph{promotion} of objects had not been available to
language-level implementers of \ac{cop}. However, at the time of writing, one
\ac{vm} with meta-tracing \ac{jit} compiler (PyPy) exposes the \emph{promote}
functionality to the language-level and it is possible to use it for a \ac{cop}
implementation. The composition of active layers can now be promoted and the
meta-tracing \ac{jit} compiler now ensures that (a) a specialized traces exists
for each encountered composition, and (b) within a given trace, the composition
wont change and can be relied upon. This assumption now can be made when
exercising lookup during execution, saving execution time.


\section{<<Impl: ContextPy, VM interface, applevel promote>>}


\section{Performance Evaluation with \acs{cop}}


As described in the introduction, we evaluate the following:
\begin{enumerate}
\item Sideways composition \emph{still} has a considerable impact on execution
  time.
\label{item:1}
\item Meta-tracing \ac{jit} compilers can alleviate the performance impact of
  sideways composition on execution time.
\label{item:2}
\item Explicit \emph{promotion} of the active layer composition can further
  improve execution time.
\label{item:3}
\end{enumerate}
%
For \ref{item:1}, we re-run in parts benchmarks presented in
2009~\cite{appeltauer+:2009:comparison-context-oriented}, which showed a
performance impact of sideways composition as used with \ac{cop}
implementations. For \ref{item:2}, we additionally augment the well-known
DeltaBlue benchmark~\cite{freeman-benson+:1989:deltablue-algorithm:} with
layers and additional functionality and compare the impact of sideways
composition on platforms with and without meta-tracing \ac{jit} compilers.
For \ref{item:3}, we run both benchmarks with a \emph{promote}-enhanced
\ac{cop} implementation, as well.
We present and discuss the most important results for each benchmark and
provide all results in the appendix.

\subsection{Setup}
\label{sec:setup}


\paragraph{System} We executed the benchmarks on %
% MacBook Pro Tobi
an Intel Core i7-4850HQ at \SI{2.3}{\GHz} with \SI{6}{\mega\byte} cache and
\SI{16}{\giga\byte} of RAM. The machine ran Mac OS\,X 10.9.5.
%%%% Surface Tim??
Certain benchmarks were run on an Intel Core i7-4650U at \SI{1.7}{\GHz} with
\SI{4}{\mega\byte} cache and \SI{8}{\giga\byte} of RAM, this machine ran
Windows 10.
%
%%%% Maxinchen
% an Intel Xeon E5-2650 (Sandy Bridge)  at \SI{2.8}{\GHz} with
% \SI{20}{\mega\byte} cache and \SI{16}{\giga\byte} of RAM. 
% Although virtualized on Xen, the machine was idle.
% The machine ran Ubuntu 14.04.1 LTS with a \SI{64}{\bit} Linux 3.2.0.

\paragraph{Implementations} %

We used ContextPy\footnote{\url{https://bitbucket.org/cfbolz/contextpy/}
  revision 5155cb7.} with Python 2.7.5 and PyPy 5.1 on OS\,X and
ContextPyPy\footnote{ContextPy as above but with explicit \emph{promote}.} with
PyPy 5.1 on OS\,X in both benchmarks. Additionally, for the re-run of the 2009
benchmark, we used
ContextJS~\cite{lincke+:2011:open-implementation}\footnote{\url{https://github.com/LivelyKernel/LivelyKernel/tree/master/core/cop}
  revision 0bd117c.} with V8 (Chrome 50.0.2661.66 beta 64-bit) on OS\,X, Chakra
(Edge 25.10586) on Windows 10 and V8 (Chrome 49.0.2623.112) on Windows 10; as
well as LispWorks\textsuperscript{\textregistered} 64-bit 7.0.0 on OS\,X.

Introducing layers with ContextPy increases the initial size of the traces
beyond PyPy's standard maximum trace limit. Therefore, we use a slightly
altered build of PyPy 5.1 that allows a much higher trace limit than the
standard build.


\paragraph{Methodology} 
% INITSIZE = 200000
% MAXSIZE = 1000000000
% TARGETTIME = 5.0 * 1000
For \copa and \copb (see \autoref{sec:benchmarks}), every benchmark was run with increasing size until a
measurement took at least 5 seconds; this matches the original
methodology~\cite{appeltauer+:2009:comparison-context-oriented}. Warm-up is
provided by the not measured runs. 
% ITER_DEFAULT=${ITER:-5}
% SIZE_DEFAULT=${SITE:-18000}
% WARM_DEFAULT=${WARM:-3}
For \deltablue (see \autoref{sec:benchmarks}), every benchmark was run 5 times
uninterrupted in a new process with additional 3 times warm-up prior to
measurement for PyPy\footnote{Python did not exhibit any warm-up--related
  differences}. The execution time was always measured \emph{in-system} and,
hence, does not include start-up. We show the arithmetic mean of all runs along
with bootstrapped~\cite{davison+:1997:confidence-intervals} confidence
intervals for a \SI{95}{\percent} confidence level.

%\footnote{Raw figures can be  found as an appendix (\autoref{tab:all})}

% \paragraph{Availability}  All of our benchmarks and infrastructure
%  are available at <<TBD>>.


\subsection{Benchmarks}
\label{sec:benchmarks}

Our first set of benchmarks is taken from one of our earlier
papers~\cite{appeltauer+:2009:comparison-context-oriented}. These
micro-benchmarks attempt to measure the pure performance overhead of
dispatching to active layers and of layer activation. In the first part of
these benchmarks (\copa), we use an object with ten integer variables
(\texttt{counter\textsubscript{1}} to \texttt{counter\textsubscript{10}}) that
provides ten methods (\texttt{method\textsubscript{1}} to
\texttt{method\textsubscript{10}}), where each \texttt{method\textsubscript{i}}
increments all counters from \texttt{counter\textsubscript{1}} to
\texttt{counter\textsubscript{i}} by one. The same behavior is provided by a
method \texttt{layered}. The base method increments only
\texttt{counter\textsubscript{1}}, and nine layers
(\texttt{Layer\textsubscript{1}} to \texttt{Layer\textsubscript{9}}) provide a
partial method to adapt the base method to each increment one of the other
counters. Running just the \texttt{layered} method without any layers being
active thus yields the same behavior as \texttt{method\textsubscript{1}}.


In the second part (\copb) of our first benchmark set, we measure the
performance impact of layer activation. For most \ac{cop} languages, layer
activation means updating internal data structures with the current layer
composition. To quantify this impact, we measured the execution time of running
five methods (\texttt{method\textsubscript{1}} to
\texttt{method\textsubscript{5}}) in succession that each increment one
counter. We compare this to the execution time of five partial methods from
five layers that implement the same method body, where each layer is activated
in succession.


Our second benchmark (\deltablue) measures the relative overhead of sideways
composition on a more wide-spread benchmark, DeltaBlue. For that, we augmented
DeltaBlue with additional functionality (reversed lists for storage, count of
constraint executions and binary constraints), both statically added to the
benchmark (Delta\emph{Purple}) and dynamically composed via layers
(Delta\emph{Red}). To measure the overhead of the mere presence of dynamic
sideways composition functionality without actually using it, we also measured
DeltaRed with all layers deactivated (Delta\emph{Violet}). We normalize the
execution time of all four benchmarks to Delta\emph{Blue} on each
implementation and \ac{vm}. However, we are not interested in the general
performance improvements of meta-tracing \ac{jit} compilers compared with
traditional \acp{vm} or \ac{jit} compilers.

All benchmarks are single-threaded.

\subsection{Results}

\begin{figure*}[htb]
  \centering
  \hbox to \linewidth{\hss
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-3}
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-2}
  \includegraphics[height=6\baselineskip]{bench/malte-a/malte-a-4}
  \hss}
  \caption{\copa}
  \label{fig:malte-a}
\end{figure*}

The results of the \copa benchmark are shown in \autoref{fig:malte-a}. We evaluated
the performance by comparing each ordinary method \texttt{method\textsubscript{i}} with the
execution performance of activating all layers from \texttt{Layer\textsubscript{1}} to
\texttt{Layer\textsubscript{i}} (which gives the same behavior) and normalizing to the
ordinary method. As we reported in our previous work, ContextL shows a
performance degradation ranging from \SIrange{22}{65}\percent. Interestingly, the decrease
in performance does not seem to correspond to the number of active layers,
likely due to the variability of the optimizations of the underlying Lisp
\ac{vm}. On the V8 and Chakra JavaScript \acp{vm}, ContextJS incurs a massive
performance hit of over 99.7\% in all cases (even where no layer is
active). ContextPy on the Python \ac{vm} is little better with overhead around
95\%.

% TODO ....
Running ContextPy on the meta-tracing PyPy \ac{vm} ...



The results of this benchmark are shown in \autoref{fig:malte-b}. The
performance impact for layer activation for each of the tested systems is
comparable and clearly increases as more layers are activated.
% TODO ....

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\linewidth]{bench/malte-b}
  \caption{\copb}
  \label{fig:malte-b}
\end{figure*}

\def\idBox#1#2{%
\setlength{\fboxsep}{1pt}%
% \colorbox[HTML]{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\colorbox{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\xspace}


DeltaBlue~\idBox{blue}{\(\circ\)}, %
DeltaPurple ~\idBox{purple}{\(\bigtriangleup\)}, %
DeltaViolet ~\idBox{violet}{\(\square\)}, %
DeltaRed~\idBox{red}{\(+\)}, %
 % ~\idBox{}{\(\boxtimes\)}, %
 % ~\idBox{E41A1C}{\(\)}, %


\begin{figure*}
  \centering
  \includegraphics[width=.7\linewidth]{bench/DeltaBlue-norm.pdf}
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue-norm.pdf}
  % \hfil
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue.pdf}
  \caption{\deltablue}
\end{figure*}

\subsection{Threats to Validity}

OSX: SpeedStep cannot be disabled.

\section{Related Work}

\section{<<Conclusion/FW: Promising first results, application to other stuff necessary>>}

\acks
We gratefully acknowledge the financial support of HPI's Research School and
the Hasso Plattner Design Thinking Research Program (HPDTRP).
% Carl Friedrich
% Bolz is supported by the EPSRC \emph{Cooler} grant EP/K01790X/1.
LispWorks Ltd. kindly provided an evaluation license of
LispWorks\textsuperscript{\textregistered} 64-bit for Mac OS\,X for testing
purposes. We greatly appreciate the support of Carl Friedrich Bolz with the
ContextPy and ContextPyPy implementations.
\printbibliography
\appendix
% \section{Appendix Title}
\input{deltablue-raw}
\input{malte-raw}
\end{document}
