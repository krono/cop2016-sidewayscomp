%
%  Fast Sideways Composition, for COP 2016
%
% Confirmation Number:» 
% Submission Passcode:» 
% MAX 6 PAGES
%
\RequirePackage[l2tabu, orthodox]{nag}
\PassOptionsToPackage{final}{graphics}
\documentclass[preprint,english,10pt,nonatbib]{sigplanconf}
\usepackage{myheader}
\addbibresource{references.bib}
\newacronym{api}{\textsc{api}}{application programming interface}
\newacronym{aop}{\textsc{aop}}{aspect-oriented programming}
\newacronym{cop}{\textsc{cop}}{context-oriented programming}
\newacronym{oop}{\textsc{oop}}{object-oriented programming}
\newacronym{oo}{\textsc{oo}}{object-oriented}
\newacronym{dsl}{\textsc{dsl}}{domain-specific language}
\newacronym{jit}{\textsc{jit}}{just-in-time}
\newacronym{vm}{\textsc{vm}}{virtual machine}
\newacronym{ffi}{\textsc{ffi}}{foreign-function interface}

\begin{document}
\conferenceinfo{COP '16}{July 19, 2016, Rome, Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/16/07}
\copyrightdoi{nnnnnnn.nnnnnnn}

\publicationrights{licensed}     % this is the default

\titlebanner{DRAFT \--- not for distribution \--- DRAFT}        % These are ignored unless
\preprintfooter{Submitted to COP'16}

\title{Optimizing Sideways Composition}
\subtitle{Fast Context-oriented Programming in ContextPyPy}

% \authorinfo{Tobias Pape}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tobias.pape@hpi.uni-potsdam.de}
% \authorinfo{Tim Felgentreff}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tim.felgentreff@hpi.uni-potsdam.de}
% \authorinfo{Carl Friedrich Bolz}
%            {King's College, London}
%            {cfbolz@gmx.de}
\authorinfo{Tobias Pape\textsuperscript{*} \and Tim Felgentreff\textsuperscript{*\textdagger} \and Robert Hirschfeld\textsuperscript{*\textdagger}}
           {\textsuperscript{*}Hasso Plattner Institute, University of Potsdam, Germany;\\
           \textsuperscript{\textdagger}Communications Design Group (CDG) , SAP Labs, USA; \\
           \textsuperscript{\textdagger}Viewpoints Research Institute, USA}
           {\{firstname\}.\{lastname\}@hpi.uni-potsdam.de}

\maketitle

\begin{abstract}
  \begin{itemize}
  \item Typical object systems only support static, single inheritance, both
    are limiting.
  \item Sideways composition provides a way of reducing their limitations.
  \item COP notably applies SC for good modularity.
  \item Most COP implementations have a substantial performance overhead.
  \item This is because weaving and execution of layered methods violate common
    assumptions about lookup.
  \item Meta-tracing JITs have unique characteristics that can alleviate the
    performance overhead.
  \item They don't need to make as strong assumptions as traditional execution
    environments have to.
  \item We postulate that MT-JITs are good at optimizing sideways composition
    and give initial results.
  \item We show that hints for the JIT compiler in the COP implementation can
    improve performance further.
  \end{itemize}
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\keywords
keyword1, keyword2

\section{Introduction}

\paragraph*{Contributions}

\begin{itemize}
\item <<We show that metatracing not only makes python fast, but also COP/SC
  faster>>
\item <<We show that \emph{promote}(TBD) can further improve stuff and
  elimitates COP overhead when COP Layers are present but not in use>>
\end{itemize}

\section{<<Background: COP, COPSurvey, MetaTracing>>}

\subsection{The Performance of COP}
\label{sec:performance-cop}
\ac{cop} as a modularity mechanism to dynamically adapt behavior at run-time has
been demonstrated to be useful in a variety of scenarios. Beyond its original
motivation for dynamically adapting systems based on environmental factors such
as battery level, geolocation, or time of day~\todo{cite}, \ac{cop} has also
been applied to provide safety in the development of live
systems~\cite{lincke+:2012:scoping-changes} or to let multiple conflicting
versions of programming interfaces co-exist~\todo{cite my talk cop 2013}. 

However, as with many abstraction mechanisms, \ac{cop} comes with some overhead,
a fact that has been repeatedly recognized. Context layer aware method lookup
requires additional operations at run-time. Most \ac{cop} extensions to dynamic
languages use the host languages meta-programming facilities to redirect method
dispatch, whereas statically compiled languages require additional compilation
steps to construct data structures to track layer activation states at
run-time. Both of these solutions come with considerable performance decrease
from 75 \% up to 99 \%~\cite{appeltauer+:2009:comparison-context-oriented}.

So far, attempts to apply traditional compiler and optimization techniques have
proven to be difficult and can only reduce the performance overhead in some
cases. A common approach is to shift the performance impact to the layer
activation time. ContextAmber~\cite{springer2015efficient} optimistically
flattens layered methods when the layer composition changes to achieve near
native performance during execution. An extension to the C++ configuration
management system Elektra~\cite{Raab:2014:PEE:2637066.2637074} make use of
extensive code generation and caching of the active layer composition to
minimize the performance impact of running with active layers in a tight
loop. Both approaches nearly eliminate the performance impact of layers when the
composition changes rarely, but at the cost of reduced performance for
applications where the layer composition may change frequently. \ac{vm}
facilities such as Java's INVOKEDYNAMIC instruction provide only minimally
improved performance compared to an implementation using language-level caching
facilities, albeit using less code and a simplified architecture.

\subsection{Meta-Tracing JIT}
\label{sec:meta-tracing-jit}

\section{<<Solution: Sideways-composition and metatracing>>}

We propose that meta-tracing \ac{jit} compilers can reduce the overhead of
sideways composition and \ac{cop}.

\Acl{cop} employs sideways composition to inject context-dependent behavioral
changes into an existing hierarchy of behavior. These hierarchies are typically
defined by the static and/or single inheritance of object systems. These
hierarchies are typically important for execution time performance, as they
form the basis for \emph{lookup}.

Most execution environments, such as \acp{vm} for dynamic object-oriented
languages, assume that those hierarchies change rarely and hence lookup can be
fast. However, using sideways composition to alter behavior invalidates this
assumption. Especially, since \ac{cop} explicitly redefines lookup based on
currently active layers; the composition of currently active layers becomes
\emph{crucial} for calculating the lookup in the dynamic extent of executed
code. If this composition stays the same, lookup stays the same, if it changes,
lookup may change. Execution environments typically have to decide, whether to
always re-exercise the lookup for every method under the active layers, or
cache (and invalidate) lookup information when the composition of active layers
changes. For example in the sequence
\begin{lstlisting}
-- active layers: §\(\emptyset\)§
method1()
  activate(layer1)
    method2()
  deactivate(layer1)
\end{lstlisting}
a \ac{cop} implementation typically uses one of the following two strategies:
\begin{enumerate}
\item run the lookup for \lstinline|method1()| under the active layers
  \(\emptyset\) and the lookup for \lstinline|method()| under the active layers
  \([\)\lstinline|layer1|\(]\), or
\item use cached lookup information for \lstinline|method1()|, switch cached
  lookup information due to change in active layers, use (new) cached lookup
  information for \lstinline|method2()|.
\end{enumerate}
Both cases produces a performance impact either on every lookup or on every
layer change.

With meta-tracing \ac{jit} compilers, however, this effect is much less
severe. Although rarely-changing lookup still is helpful for its operation, a
change in lookup \--- for example induced by a layer activation \--- can be
anticipated and be accounted for.

Thus, with a properly instructed meta-tracing \ac{jit} compiler, a third option
becomes available. At points in the execution where the composition of active
layers becomes important, a \emph{guard} ensures that this composition did not
change. While counter-intuitive at first, this actually is a benefit. When a
certain different composition has been encountered often enough at the guard,
the meta-tracing \ac{jit} compiler will introduce a \emph{bridge} into a new
part of a trace, in which \emph{this} different composition can be assumed not
to change, and lookup can be optimized accordingly. Note that this resembles
strategy 2 above, but is implicit and actually guided by the \ac{jit} compiler.
Therefore, the actual \ac{cop} implementation does not have to manage the
caching information when altering the lookup information, saving both execution
time and implementation complexity.

\paragraph{Promoted layer compositions} 

\iffalse
% Frm that paper DO NOT iftrue.
Promotion is essentially a tool for trace specialization. There are places in
the interpreter where it would open a lot of optimization opportunities if a
variable were constant, even though it could have different values in practice.
In such a place, promotion can be used.

The typical reason to do that is if there is a lot of computation depending on
the value of one variable.

Let us make this more concrete. If we trace a call to the function (written in
RPython) on the left, we get the trace on the right:

Observe how the first two operations could be constant-folded if the value of
x1 were known. Let us assume that the value of x in the RPython code can vary,
but does so rarely, i.e. only takes a few different values at runtime. If this
is the case, we can add a hint to promote x, like this

The hint indicates that x is likely a runtime constant and the JIT should try
to perform runtime specialization on it in the code that follows. When just
running the code, the promote function has no effect. When tracing, some extra
work is done. Let us assume that this changed function is traced with the
arguments 4 and 8. The trace will be the same, except for one operation at the
beginning.

The promotion is turned into a guard operation in the trace. The guard captures
the runtime value of x as it was during tracing, which can then be exploited by
the compiler. The introduced guard specializes the trace, because it only works
if the value of x1 is 4. From the point of view of the optimizer, this guard is
not different frome the one produced by the if statement in the first example.
After the guard, it can be assumed that x1 is equal to 4, meaning that the
optimizer will turn this trace into:

guard(x1 == 4) v2 =9+y1 return(v2 )

Notice how the first two arithmetic operations were constant folded. The hope
is that the guard is executed quicker than the multiplication and the addition
that was now optimized away.

If this trace is executed with values of x1 other than 4, the guard will fail,
and execution will continue in the interpreter. If the guard fails often
enough, a new trace will be started from the guard. This other trace will
capture a different value of x1. If it is e.g. 2, then the optimized trace
looks like this:

guard(x1 == 2) v2 =5+y1 return(v2 )


This new trace will be attached to the guard instruction of the first trace. If
x1 takes on even more values, a new trace will eventually be made for all of
them, linking them into a chain. This is clearly not desirable, so we should
promote only variables that do not vary much. However, adding a promotion hint
will never produce wrong results. It might just lead to too much machine code
being generated.

Promoting integers, as in the examples above, is not used that often. However,
the internals of dynamic language interpreters of- ten have values that are
variable but vary little in the context of parts of a user program. An example
would be the types of vari- ables in a user function, which rarely change in a
dynamic lan- guage in practice (even though they could). In the interpreter,
these user-level types are values. Thus promoting them will lead to
type-specialization on the level of the user program. Section 4 will present a
complete example of how this works.
\fi


~\cite{bolz+:2011:runtime-feedback}


\section{<<Impl: ContextPy, VM interface, applevel promote>>}


\section{<<Eval: Benchmarks>>}


\def\idBox#1#2{%
\setlength{\fboxsep}{1pt}%
\colorbox[HTML]{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\xspace}



We evaluate... hypotheses..., as described in the introduction:
\begin{enumerate}
\item Sideways composition \emph{still} has a considerable impact on execution
  time. <<still<-> copsurvey>>
\item Tracing JITs can alleviate the performance impact of sideways composition.
\item <<applevel promote>>.
\end{enumerate}



We tested the first hypothesis by ....

We tested the second hypothesis by ....

We tested the third hypothesis by ....

% PYCKET-COPY - DO NOT USE VERBATIM
% Our evaluation compares \pycket with multiple configurations and systems on
% a variety of programs. We present the most important results and include full
% results in supplemental material.

\subsection{Setup}
\label{sec:setup}


\paragraph{System} We conducted the experiments on %
% MacBook Pro Tobi
an Intel Core i7-4850HQ at \SI{2.3}{\GHz} with \SI{6}{\mega\byte} cache and
\SI{16}{\giga\byte} of RAM. The machine ran Mac OS\,X 10.9.5.
%%%% Surface Tim??
%
%%%% Maxinchen
% an Intel Xeon E5-2650 (Sandy Bridge)  at \SI{2.8}{\GHz} with
% \SI{20}{\mega\byte} cache and \SI{16}{\giga\byte} of RAM. 
% Although virtualized on Xen, the machine was idle.
% The machine ran Ubuntu 14.04.1 LTS with a \SI{64}{\bit} Linux 3.2.0.
RPython as of revision
648874ef8243
 was used for <<pypypromote>>.

All benchmarks are single-threaded.


\paragraph{Implementations} %
~\idBox{377EB8}{\(\circ\)}, %
~\idBox{4DAF4A}{\(\bigtriangleup\)}, %
~\idBox{984EA3}{\(\square\)}, %
~\idBox{FF7F00}{\(+\)}, %
~\idBox{FF7F00}{\(\boxtimes\)}, %
~\idBox{E41A1C}{\(\)}, %

<<DeltaBlue/Violet/Red>>
\begin{itemize}
\item ContextPy with Python 2.7.5 on OS\,X
\item ContextPy with PyPy 5.0.1 on OS\,X
\item ContextPy with PyPyPromote (5.1.0-alpha0 648874ef8243) on OS\,X
\end{itemize}

and additionally for <<maltebench>>
\begin{itemize}
\item ContextJS with Chrome 50.0.2661.66 beta (64-bit) on OS\,X
\item << ContextJS Win10 Edge -- tim? >>
\item <<ContextJS Win10 Chrome -- tim?>>
\item <<(ContextJS Win10 Firefox -- tim?)>>
\end{itemize}

\paragraph{Methodology} 
% INITSIZE = 200000
% MAXSIZE = 1000000000
% TARGETTIME = 5.0 * 1000
For <<maltebench>>, every benchmark was with increasing size until a
measurement took at least 5 seconds; this matches the original
methodology~\cite{appeltauer+:2009:comparison-context-oriented}.Warm-up is
provided by the not measured runs.
% ITER_DEFAULT=${ITER:-5}
% SIZE_DEFAULT=${SITE:-20000}
% WARM_DEFAULT=${WARM:-2}
For <<DeltaBlue>>, every benchmark was run 5 times uninterrupted at highest
priority in a new process with additional 2 times prior to measurement.
The execution time was always measured \emph{in-system} and, hence, does not include start-up.
We show the arithmetic mean of all  runs along with
bootstrapped~\cite{davison+:1997:confidence-intervals} confidence intervals
for a \SI{95}{\percent} confidence level.

%\footnote{Raw figures can be  found as an appendix (\autoref{tab:all})}

\paragraph{Availability}  All of our benchmarks and infrastructure
 are available at <<TBD>>.


\subsection{Benchmarks}


\subsection{Threats to Validity}

OSX: SpeedStep cannot be disabled.

\section{Related Work}

\section{<<Conclusion/FW: Promising first results, application to other stuff necessary>>}

\acks
We gratefully acknowledge the financial support of HPI's Research School and
the Hasso Plattner Design Thinking Research Program (HPDTRP).
% Carl Friedrich
% Bolz is supported by the EPSRC \emph{Cooler} grant EP/K01790X/1.
LispWorks Ltd. kindly provided an evaluation license of
LispWorks\textsuperscript{\textregistered} 64-bit for Mac OS\,X for testing
purposes.

\printbibliography
% \appendix
% \section{Appendix Title}

\end{document}
