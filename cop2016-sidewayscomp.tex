%
%  Fast Sideways Composition, for COP 2016
%
% Confirmation Number:» 9
% Submission Passcode:» n/a
% MAX 6 PAGES
%
\RequirePackage[l2tabu, orthodox]{nag}
\PassOptionsToPackage{final}{graphics}
\documentclass[preprint,english,10pt,nonatbib]{sigplanconf}
\usepackage{myheader}
\addbibresource{references.bib}
\newacronym{api}{\textsc{api}}{application programming interface}
\newacronym{aop}{\textsc{aop}}{aspect-oriented programming}
\newacronym{cop}{\textsc{cop}}{context-oriented programming}
\newacronym{oop}{\textsc{oop}}{object-oriented programming}
\newacronym{oo}{\textsc{oo}}{object-oriented}
\newacronym{dsl}{\textsc{dsl}}{domain-specific language}
\newacronym{jit}{\textsc{jit}}{just-in-time}
\newacronym{vm}{\textsc{vm}}{virtual machine}
\newacronym{ffi}{\textsc{ffi}}{foreign-function interface}
\newcommand*\copa{\textsc{cop}\,\oldstylenums{09}\,a\xspace}
\newcommand*\copb{\textsc{cop}\,\oldstylenums{09}\,b\xspace}
\newcommand*\deltablue{\textsc{DeltaBlue}\xspace}

\begin{document}
\conferenceinfo{COP '16}{July 19, 2016, Rome, Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/16/07}
\copyrightdoi{nnnnnnn.nnnnnnn}

\publicationrights{licensed}     % this is the default

\titlebanner{DRAFT \--- not for distribution \--- DRAFT}        % These are ignored unless
\preprintfooter{Submitted to COP'16}

\title{Optimizing Sideways Composition}
\subtitle{Fast Context-oriented Programming in ContextPyPy}

% \authorinfo{Tobias Pape}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tobias.pape@hpi.uni-potsdam.de}
% \authorinfo{Tim Felgentreff}
%            {Hasso Plattner Institute\\ University of Potsdam, Germany}
%            {tim.felgentreff@hpi.uni-potsdam.de}
% \authorinfo{Carl Friedrich Bolz}
%            {King's College, London}
%            {cfbolz@gmx.de}
\authorinfo{Tobias Pape\textsuperscript{*} \and Tim Felgentreff\textsuperscript{*\textdagger} \and Robert Hirschfeld\textsuperscript{*\textdagger}}
           {\textsuperscript{*}Hasso Plattner Institute, University of Potsdam, Germany\\
           \textsuperscript{\textdagger}Communications Design Group (CDG), SAP Labs, USA\\
           \textsuperscript{\textdagger}Viewpoints Research Institute, USA}
           {\{firstname\}.\{lastname\}@hpi.uni-potsdam.de}

\maketitle

\begin{abstract}
  The prevalent way of code sharing in most of the current object systems is
  static and/or single inheritance, both are limiting. Sideways composition
  provides a technique to reduce their limitations. \Ac{cop} notably applies
  sideways composition to achieve better modularity. However, most \ac{cop}
  implementations have a substantial performance overhead. This is partly
  because weaving and execution of layered methods violate common assumptions
  about lookup. Meta-tracing \ac{jit} compiler have unique characteristics that
  can alleviate the performance overhead, as they can treat lookup
  differently.\\
  We suggest that meta-tracing \ac{jit} compilers are good at optimizing
  sideways composition and give initial results. Furthermore, we show that
  hints for the \ac{jit} compiler in a \ac{cop} implementation can improve
  performance further.
  \end{abstract}

\category{D.3.3}{Programming Languages}{Language Constructs and Features}
\category{D.3.2}{Programming Languages}{Language Classifications}[Mul\-ti\-par\-a\-digm languages]
\category{D.3.4}{Programming Languages}{Processors}[Run-time environments]

\keywords
context-oriented programming,
meta-tracing \acs{jit} compilers,
optimization,
virtual machines,
PyPy

\section{Introduction}

\paragraph*{Contributions}

\begin{itemize}
\item <<We show that metatracing not only makes python fast, but also COP/SC
  faster>>
\item <<We show that \emph{promote}(TBD) can further improve stuff and
  elimitates COP overhead when COP Layers are present but not in use>>
\end{itemize}

\section{<<Background: COP, COPSurvey, MetaTracing>>}

\subsection{The Performance of COP}
\label{sec:performance-cop}
\ac{cop} as a modularity mechanism to dynamically adapt behavior at run-time has
been demonstrated to be useful in a variety of scenarios. Beyond its original
motivation for dynamically adapting systems based on environmental factors such
as battery level, geolocation, or time of day~\todo{cite}, \ac{cop} has also
been applied to provide safety in the development of live
systems~\cite{lincke+:2012:scoping-changes} or to let multiple conflicting
versions of programming interfaces co-exist~\todo{cite my talk cop 2013}.

~\cite{lincke+:2011:implementing-scoped}

However, as with many abstraction mechanisms, \ac{cop} comes with some overhead,
a fact that has been repeatedly recognized. Context layer aware method lookup
requires additional operations at run-time. Most \ac{cop} extensions to dynamic
languages use the host languages meta-programming facilities to redirect method
dispatch, whereas statically compiled languages require additional compilation
steps to construct data structures to track layer activation states at
run-time. Both of these solutions come with considerable performance decrease
from \SIrange{75}{99}\percent~\cite{appeltauer+:2009:comparison-context-oriented}.

So far, attempts to apply traditional compiler and optimization techniques have
proven to be difficult and can only reduce the performance overhead in some
cases. A common approach is to shift the performance impact to the layer
activation time. ContextAmber~\cite{springer2015efficient} optimistically
flattens layered methods when the layer composition changes to achieve near
native performance during execution. An extension to the C++ configuration
management system Elektra~\cite{Raab:2014:PEE:2637066.2637074} make use of
extensive code generation and caching of the active layer composition to
minimize the performance impact of running with active layers in a tight
loop. Both approaches nearly eliminate the performance impact of layers when the
composition changes rarely, but at the cost of reduced performance for
applications where the layer composition may change frequently. \ac{vm}
facilities such as Java's INVOKEDYNAMIC instruction provide only minimally
improved performance compared to an implementation using language-level caching
facilities, albeit using less code and a simplified architecture.

\subsection{Meta-Tracing JIT}
\label{sec:meta-tracing-jit}

\section{<<Solution: Sideways-composition and metatracing>>}

We propose that meta-tracing \ac{jit} compilers can reduce the overhead of
sideways composition and \ac{cop}, and that \emph{promoting} the composition of
active layers can reduce the overhead even further.

\subsection{Employing a meta-tracing \protect\acs{jit}}
\Acl{cop} employs sideways composition to inject context-dependent behavioral
changes into an existing hierarchy of behavior. These hierarchies are typically
defined by the static and/or single inheritance of object systems. These
hierarchies are typically important for execution time performance, as they
form the basis for \emph{lookup}.

Most execution environments, such as \acp{vm} for dynamic object-oriented
languages, assume that those hierarchies change rarely and hence lookup can be
fast. However, using sideways composition to alter behavior invalidates this
assumption. Especially, since \ac{cop} explicitly redefines lookup based on
currently active layers; the composition of currently active layers becomes
\emph{crucial} for calculating the lookup in the dynamic extent of executed
code. If this composition stays the same, lookup stays the same, if it changes,
lookup may change. Execution environments typically have to decide, whether to
always re-exercise the lookup for every method under the active layers, or
cache (and invalidate) lookup information when the composition of active layers
changes. For example in the sequence
\begin{lstlisting}
-- active layers: §\(\emptyset\)§
method1()
  activate(layer1)
    method2()
  deactivate(layer1)
\end{lstlisting}
a \ac{cop} implementation typically uses one of the following two strategies:
\begin{enumerate}
\item run the lookup for \lstinline|method1()| under the active layers
  \(\emptyset\) and the lookup for \lstinline|method()| under the active layers
  \([\)\lstinline|layer1|\(]\), or
\item use cached lookup information for \lstinline|method1()|, switch cached
  lookup information due to change in active layers, use (new) cached lookup
  information for \lstinline|method2()|.
\end{enumerate}
Both cases effect a performance impact either on every lookup or on every
layer change.

With meta-tracing \ac{jit} compilers, however, this effect is much less
severe. Although rarely-changing lookup still is helpful for its operation, a
change in lookup \--- for example induced by a layer activation \--- can be
anticipated and be accounted for.

Thus, with a properly instructed meta-tracing \ac{jit} compiler, a third option
becomes available. At points in the execution where the composition of active
layers becomes important, a \emph{guard} ensures that this composition did not
change. While counter-intuitive at first, this actually is a benefit. When a
certain different composition has been encountered often enough at the guard,
the meta-tracing \ac{jit} compiler will introduce a \emph{bridge} into a new
part of a trace, in which \emph{this} different composition can be assumed not
to change, and lookup can be optimized accordingly. Note that this resembles
strategy 2 above, but is implicit and actually guided by the \ac{jit} compiler.
Therefore, the actual \ac{cop} implementation does not have to manage the
caching information when altering the lookup information, saving both execution
time and implementation complexity.

\subsection{Promoting the compositions of active layers}

The compositions of active layers is crucially important for the lookup in
\ac{cop}, and all strategies above reflect this. However, only the
language-level implementation of \ac{cop} actually knows about the
composition's importance, and even for meta-tracing \ac{jit} compiler's
strategy, the \ac{jit} compiler first hast to become aware of the fact that the
composition is \--- indeed \--- important for its trace. Yet, the \ac{jit}
compile can only apply heuristics to identify the composition as
trace-important.

This situation is commonly known when implementing \acp{vm} that use a
meta-tracing \ac{jit} compiler. Based on the value of a certain object it may
be desirable to \emph{specialize} traces to these values (essentially what
happened above with the guard and the bridge).
Implementers can chose to \emph{promote}~\cite[\S
3.1]{bolz+:2011:runtime-feedback} such an object and the \ac{jit} compiler will
ensure that traces are actually specialized to the object's values,  regardless
of wether the \ac{jit} compiler's \emph{heuristics} would result in the same
specialization or not. If applied carefully, this promotion can decrease
execution time.

Up until recently, this \emph{promotion} of objects had not been available to
language-level implementers of \ac{cop}. However, at the time of writing, one
\ac{vm} with meta-tracing \ac{jit} compiler (PyPy) exposes the \emph{promote}
functionality to the language-level and it is possible to use it for a \ac{cop}
implementation. The composition of active layers can now be promoted and the
meta-tracing \ac{jit} compiler now ensures that (a) a specialized traces exists
for each encountered composition, and (b) within a given trace, the composition
wont change and can be relied upon. This assumption now can be made when
exercising lookup during execution, saving execution time.


\section{<<Impl: ContextPy, VM interface, applevel promote>>}


\section{<<Eval: Benchmarks>>}




We evaluate... hypotheses..., as described in the introduction:
\begin{enumerate}
\item Sideways composition \emph{still} has a considerable impact on execution
  time. <<still<-> copsurvey>>
\item Tracing JITs can alleviate the performance impact of sideways composition.
\item <<applevel promote>>.
\end{enumerate}



We tested the first hypothesis by ....

We tested the second hypothesis by ....

We tested the third hypothesis by ....

% PYCKET-COPY - DO NOT USE VERBATIM
% Our evaluation compares \pycket with multiple configurations and systems on
% a variety of programs. We present the most important results and include full
% results in supplemental material.

\subsection{Setup}
\label{sec:setup}


\paragraph{System} We conducted the experiments on %
% MacBook Pro Tobi
an Intel Core i7-4850HQ at \SI{2.3}{\GHz} with \SI{6}{\mega\byte} cache and
\SI{16}{\giga\byte} of RAM. The machine ran Mac OS\,X 10.9.5.
%%%% Surface Tim??
%
%%%% Maxinchen
% an Intel Xeon E5-2650 (Sandy Bridge)  at \SI{2.8}{\GHz} with
% \SI{20}{\mega\byte} cache and \SI{16}{\giga\byte} of RAM. 
% Although virtualized on Xen, the machine was idle.
% The machine ran Ubuntu 14.04.1 LTS with a \SI{64}{\bit} Linux 3.2.0.

\paragraph{Implementations} %

<<DeltaBlue/Violet/Red>>
\begin{itemize}
\item ContextPy with Python 2.7.5 on OS\,X
\item ContextPy with PyPy 5.1 on OS\,X
\item ContextPyPy with PyPy 5.1 on OS\,X
\end{itemize}

and additionally for <<maltebench>>
\begin{itemize}
\item ContextJS with Chrome 50.0.2661.66 beta (64-bit) on OS\,X
\item << ContextJS Win10 Edge -- tim? >>
\item <<ContextJS Win10 Chrome -- tim?>>
\end{itemize}

We use a slightly altered build of PyPy 5.1 that allows a much higher trace
limit than the standard build because introducing layers with ContextPy
increases the initial size of the traces beyond PyPy's standard maximum.


\paragraph{Methodology} 
% INITSIZE = 200000
% MAXSIZE = 1000000000
% TARGETTIME = 5.0 * 1000
For \copa and \copb (see \autoref{sec:benchmarks}), every benchmark was run with increasing size until a
measurement took at least 5 seconds; this matches the original
methodology~\cite{appeltauer+:2009:comparison-context-oriented}.Warm-up is
provided by the not measured runs.
% ITER_DEFAULT=${ITER:-5}
% SIZE_DEFAULT=${SITE:-18000}
% WARM_DEFAULT=${WARM:-3}
For \deltablue (see \autoref{sec:benchmarks}), every benchmark was run 5 times uninterrupted at highest
priority in a new process with additional 3 times prior to measurement for
PyPy\footnote{Python did not exhibit any warm-up-related differences}. The
execution time was always measured \emph{in-system} and, hence, does not
include start-up. We show the arithmetic mean of all runs along with
bootstrapped~\cite{davison+:1997:confidence-intervals} confidence intervals for
a \SI{95}{\percent} confidence level.

%\footnote{Raw figures can be  found as an appendix (\autoref{tab:all})}

\paragraph{Availability}  All of our benchmarks and infrastructure
 are available at <<TBD>>.


\subsection{Benchmarks}
\label{sec:benchmarks}

Our first set of benchmarks is taken from one of our earlier
papers~\cite{appeltauer+:2009:comparison-context-oriented}. These
micro-benchmarks attempt to measure the pure performance overhead of dispatching
to active layers and of layer activation.
In the first of these benchmarks (\copa), we use an object with ten integer variables
(\texttt{counter$_1$} to \texttt{counter$_{10}$}) that provides ten methods
(\texttt{method$_1$} to \texttt{method$_{10}$}), where each \texttt{method$_i$}
increments all counters from \texttt{counter$_1$} to \texttt{counter$_i$} by
one. The same behavior is provided by a method \texttt{layered}. The base method
increments only \texttt{counter$_1$}, and nine layers (\texttt{Layer$_2$} to
\texttt{Layer$_{10}$}) provide a partial method to adapt the base method to each
increment one of the other counters. Running just the \texttt{layered} method
without any layers being active thus yields the same behavior as
\texttt{method$_1$}.


In our second benchmark (\copb) we measure the performance impact of layer
activation. For most \ac{cop} languages, layer activation means updating
internal data structures with the current layer composition. To quantify this
impact, we measured the execution time of running five methods
(\texttt{method$_1$} to \texttt{method$_5$}) in succession that each increment
one counter. We compare this to the execution time of five partial methods from
five layers that implement the same method body, where each layer is activated
in succession.





All benchmarks are single-threaded.

\subsection{Results}

The results of this benchmark are shown in \autoref{fig:malte-a}. We evaluated
the performance by comparing each ordinary method \texttt{method$_i$} with the
execution performance of activating all layers from \texttt{Layer$_2$} to
\texttt{Layer$_i$} (which gives the same behavior) and normalizing to the
ordinary method. As we reported in our previous work, ContextL shows a
performance degradation ranging from 22\% to 65\%. Interestingly, the decrease
in performance does not seem to correspond to the number of active layers,
likely due to the variability of the optimizations of the underlying Lisp
\ac{vm}. On the V8 and Chakra JavaScript \acp{vm}, ContextJS incurs a massive
performance hit of over 99.7\% in all cases (even where no layer is
active). ContextPy on the Python \ac{vm} is little better with overhead around
95\%.

% TODO ....
Running ContextPy on the meta-tracing PyPy \ac{vm} ...

\begin{figure*}[htb]
  \centering
  \includegraphics[height=5.5\baselineskip]{bench/malte-a/malte-a-3}
  \includegraphics[height=5.5\baselineskip]{bench/malte-a/malte-a-2}
  \includegraphics[height=5.5\baselineskip]{bench/malte-a/malte-a-4}
  \caption{Bench A}
  \label{fig:malte-a}
\end{figure*}


The results of this benchmark are shown in \autoref{fig:malte-b}. The
performance impact for layer activation for each of the tested systems is
comparable and clearly increases as more layers are activated. 
% TODO ....

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\linewidth]{bench/malte-b}
  \caption{Bench B}
  \label{fig:malte-b}
\end{figure*}

\def\idBox#1#2{%
\setlength{\fboxsep}{1pt}%
% \colorbox[HTML]{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\colorbox{#1}{\textcolor[gray]{0.9}{\rule[0.1pt]{0pt}{5pt}#2}}%
\xspace}


DeltaBlue~\idBox{blue}{\(\circ\)}, %
DeltaPurple ~\idBox{purple}{\(\bigtriangleup\)}, %
DeltaViolet ~\idBox{violet}{\(\square\)}, %
DeltaRed~\idBox{red}{\(+\)}, %
 % ~\idBox{}{\(\boxtimes\)}, %
 % ~\idBox{E41A1C}{\(\)}, %


\begin{figure*}
  \centering
  \includegraphics[width=.7\linewidth]{bench/DeltaBlue-norm.pdf}
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue-norm.pdf}
  % \hfil
  % \includegraphics[width=.48\linewidth]{bench/DeltaBlue.pdf}
  \caption{DeltaBlue}
\end{figure*}

\subsection{Threats to Validity}

OSX: SpeedStep cannot be disabled.

\section{Related Work}

\section{<<Conclusion/FW: Promising first results, application to other stuff necessary>>}

\acks
We gratefully acknowledge the financial support of HPI's Research School and
the Hasso Plattner Design Thinking Research Program (HPDTRP).
% Carl Friedrich
% Bolz is supported by the EPSRC \emph{Cooler} grant EP/K01790X/1.
LispWorks Ltd. kindly provided an evaluation license of
LispWorks\textsuperscript{\textregistered} 64-bit for Mac OS\,X for testing
purposes.

\printbibliography
\appendix
% \section{Appendix Title}
\input{deltablue-raw}
\input{malte-raw}
\end{document}
